<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>BravoBot – Voice Assistant | Aaveg Shangari</title>
    <link rel="stylesheet" href="../style.css" />
    <style>
      .project-detail {
        max-width: 800px;
        margin: auto;
        padding: 2rem;
      }
      .project-detail h1 {
        color: #0077b6;
        margin-bottom: 0.5rem;
      }
      .project-detail h3 {
        margin-top: 2rem;
        color: #0077b6;
        font-size: 1.25rem;
      }
      .project-detail p {
        color: #444;
      }
      .project-detail img {
        width: 100%;
        border-radius: 6px;
        margin: 1rem 0;
      }
      .back-link {
        margin-top: 2rem;
        display: inline-block;
        color: #0077b6;
        text-decoration: none;
      }
      .back-link:hover {
        text-decoration: underline;
      }
      .project-detail a {
        color: #0077b6;
      }
      .project-detail a:hover {
        color: #0096c7;
      }
    </style>
  </head>
  <body>
    <div class="project-detail">
      <h1>BravoBot – Voice-Activated Personal Assistant</h1>
      <p>
        <strong>Project Type:</strong> AI Assistant · Voice + Vector Memory +
        LLM
      </p>

      <h3>Overview</h3>
      <p>
        BravoBot is a modular, voice-activated AI assistant that runs entirely
        from the command line. It transcribes user speech, classifies the intent
        using a fine-tuned transformer model, and performs actions using
        automation, memory, or generative AI. It is designed to run
        offline-first with local LLMs (like LLaMA via Ollama), and can
        optionally fall back to OpenAI when needed.
      </p>

      <h3>Motivation</h3>
      <p>
        Most voice assistants either rely heavily on cloud APIs or lack
        extensibility. BravoBot was created to be a fully offline-capable,
        modular system for power users. With built-in voice UX, persistent
        semantic memory, and plugin-style routing, it enables personal
        productivity, automation, and AI interaction — all locally.
      </p>

      <h3>Key Features</h3>
      <ul>
        <li>Custom wake word detection ("Ben") for privacy and control</li>
        <li>Intent classification using a fine-tuned DistilBERT model</li>
        <li>Offline and online LLM support (Ollama and OpenAI GPT fallback)</li>
        <li>
          Vector-based semantic memory using FAISS + sentence-transformers
        </li>
        <li>
          Memory recall through similarity search and LLM prompt injection
        </li>
        <li>
          Voice-controlled commands: Google search, YouTube, time, weather
        </li>
        <li>Note-taking, summarization, and CSV export via voice</li>
        <li>Session log summarization using AI</li>
        <li>Interruptible TTS using Shift + Q hotkey</li>
        <li>Fully modular plugin routing via context-aware handlers</li>
      </ul>

      <h3>Architecture & Workflow</h3>
      <p>
        Speech is transcribed using Whisper and passed to an intent classifier
        (DistilBERT). Based on the intent, control is routed to the appropriate
        plugin handler (e.g. vector memory, automation, LLM query). Semantic
        memory is stored locally using FAISS; top matches are retrieved based on
        the user's prompt and injected into the LLM context to enhance
        responses. All components run locally by default, with optional
        internet-based LLM fallback.
      </p>

      <h3>Technologies Used</h3>
      <p>
        Python, Whisper, DistilBERT, Hugging Face Transformers, FAISS,
        Sentence-Transformers, PyTorch, Ollama, OpenAI GPT, pyttsx3, Selenium,
        REST APIs
      </p>

      <p>
        <a href="https://github.com/AviShangari/BravoBot" target="_blank">
          View GitHub Repository
        </a>
      </p>

      <a class="back-link" href="../index.html">← Back to Portfolio</a>
    </div>
  </body>
</html>
